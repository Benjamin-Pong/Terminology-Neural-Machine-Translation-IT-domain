{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euo9bDBtNw8U",
        "outputId": "74354b86-8d1e-48b8-f390-3f024c5c7873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "9IHiLCwwFaCA",
        "outputId": "d5067329-7f91-4514-9abc-020864b0bcdb"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-6-4056942060.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-6-4056942060.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-fr.tsv.gz\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "fJhvETp9TkCK",
        "outputId": "ae7a9ca9-642c-4be6-9225-c62f79a16003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('He aquí el relato de la derrota del Markawiz ¡Alá le confunda!', 'He aquí el relato de la derrota del Markawiz ¡Alá le confunda!\\n')\n",
            "('He never fights alongside or against Gordon Freeman.', 'Nunca combate al lado o en contra de Gordon Freeman.\\n')\n",
            "('Nada más que la verdad: el juicio a las juntas.', 'Nada más que la verdad: el juicio a las juntas.\\n')\n",
            "('We live among them and rarely notice this big human fake...', 'Vivimos entre ellos y notamos raramente esa gran falsedad humana....\\n')\n",
            "('He said: \"I am four years old.', 'Dijo: «Serán mis últimos cuatro años».\\n')\n",
            "('4.', '4.')\n",
            "('The ratification and implementation of the updated ILO conventions (vote)', 'Ratificación y ampliación de los convenios actualizados de la OIT (votación)')\n",
            "('7.', '7.')\n",
            "('Sustainable agriculture and biogas: review of EU legislation (', 'La agricultura sostenible y el biogás: revisión de la legislación de la UE (')\n",
            "('- Before the vote on paragraph 41:', '- Antes de la votación del apartado 41:')\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Save the split data as JSON files\\nwith open(training_out_domain, 'w') as f:\\n    json.dump({'source': source_train, 'target': target_train}, f)\\n\\nwith open(test_out_domain, 'w') as f:\\n    json.dump({'source': source_test, 'target': target_test}, f)\\n\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from posixpath import split\n",
        "import random\n",
        "import json\n",
        "\n",
        "#### load out-domain data ######\n",
        "\n",
        "'''\n",
        "Split the data into train and test sets\n",
        "'''\n",
        "\n",
        "out_domain_repo_1 = '/content/drive/MyDrive/WikiMatrix.en-es.tsv'\n",
        "out_domain_repo_2 = '/content/drive/MyDrive//europarl-v10.es-en.tsv'\n",
        "\n",
        "all_data = []\n",
        "count1, count2 = 0, 0\n",
        "with open(out_domain_repo_1, 'r') as f, open(out_domain_repo_2, 'r') as g:\n",
        "  for line in f:\n",
        "    split_line = line.split('\\t')\n",
        "    target_sample = split_line[2]\n",
        "    source_sample = split_line[1]\n",
        "    all_data.append((source_sample, target_sample))\n",
        "    if count1<5:\n",
        "\n",
        "      print((source_sample, target_sample))\n",
        "      count1+=1\n",
        "\n",
        "  for line in g:\n",
        "    split_line = line.split('\\t')\n",
        "    target_sample = split_line[0]\n",
        "    source_sample = split_line[1]\n",
        "    all_data.append((source_sample, target_sample))\n",
        "    if count2<5:\n",
        "\n",
        "      print((source_sample, target_sample))\n",
        "      count2+=1\n",
        "\n",
        "random.shuffle(all_data)\n",
        "\n",
        "\n",
        "'''\n",
        "# Save the split data as JSON files\n",
        "with open(training_out_domain, 'w') as f:\n",
        "    json.dump({'source': source_train, 'target': target_train}, f)\n",
        "\n",
        "with open(test_out_domain, 'w') as f:\n",
        "    json.dump({'source': source_test, 'target': target_test}, f)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKOfo3sQM50J",
        "outputId": "06300706-603a-4322-f4d1-f6af851052e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6666926 1666732\n"
          ]
        }
      ],
      "source": [
        "# Split the data into train and test sets\n",
        "test_size = int(0.8 * len(all_data))\n",
        "test_data = all_data[:test_size]\n",
        "train_data = all_data[test_size:]\n",
        "print(len(test_data), len(train_data))\n",
        "\n",
        "train_data_ = train_data[:30001] #data to train the out-domain language model\n",
        "test_data_= test_data #data to retrieve more in-domain language model via CED scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hs0hHukJnOj",
        "outputId": "3f21e98f-e198-4851-b30f-3878750fe1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30001 6666926\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data_), len(test_data_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1-P7XEB4OwM"
      },
      "source": [
        "N-gram Language Modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEUyhON1M82F"
      },
      "outputs": [],
      "source": [
        "##### dump out training and testing files for n-gram lm training on patas ######\n",
        "lm_train = '/content/drive/MyDrive/lm.es.train.txt'\n",
        "lm_test= '/content/drive/MyDrive/lm.es.test.txt'\n",
        "\n",
        "with open(lm_train, 'w') as f:\n",
        "  for data in train_data_:\n",
        "    f.write(data[0].strip() + '\\t' + data[1].strip() + '\\n')\n",
        "\n",
        "with open(lm_test, 'w') as f:\n",
        "  for data in test_data_:\n",
        "      f.write(data[0].strip() + '\\t' + data[1].strip() + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd7_whWmaAtf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVTpU0XmTkbg"
      },
      "source": [
        "How much seed in-domain data do I need?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
